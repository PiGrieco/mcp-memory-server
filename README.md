# ğŸ§  SAM - Smart Access Memory

**Intelligent AI Memory Management with ML Auto-Triggers**

[![Version](https://img.shields.io/badge/version-2.1.0-blue.svg)](https://github.com/PiGrieco/mcp-memory-server)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://python.org)
[![MCP Protocol](https://img.shields.io/badge/MCP-Protocol-purple.svg)](https://modelcontextprotocol.io)
[![HuggingFace](https://img.shields.io/badge/ğŸ¤—-ML_Model-yellow.svg)](https://huggingface.co/PiGrieco/mcp-memory-auto-trigger-model)

---

## ğŸ“‹ **Table of Contents**

1. [ğŸ¯ What is SAM?](#-what-is-sam)
2. [ğŸ—ï¸ Architecture Overview](#ï¸-architecture-overview)
3. [ğŸš€ Installation](#-installation)
   - [ğŸ’¬ Prompt-Based Installation](#-prompt-based-installation-recommended)
   - [ğŸ“Š Installation Process Flow](#-installation-process-flow)
   - [ğŸ¯ Platform-Specific Commands](#-platform-specific-commands)
4. [ğŸš€ Server Modes & Operation](#-server-modes--operation)
   - [ğŸ“Š Server Operation Flow](#-server-operation-flow)
   - [ğŸ¯ Server Mode Comparison](#-server-mode-comparison)
   - [ğŸ• Watchdog Service](#-watchdog-service-auto-restart)
   - [ğŸš€ Quick Start Commands](#-quick-start-commands)
5. [âš™ï¸ How SAM Works](#ï¸-how-sam-works)
   - [ğŸ§  Technical Overview](#-technical-overview)
   - [ğŸ¯ User Benefits](#-user-benefits)
   - [ğŸ’¼ Use Cases](#-use-cases)
6. [ğŸ¤– Auto-Trigger System](#-auto-trigger-system)
   - [ğŸ§ª How the ML Model Works](#-how-the-ml-model-works)
   - [ğŸ“Š Training Dataset](#-training-dataset)
   - [ğŸ¯ Training Results](#-training-results)
   - [ğŸ”§ Hybrid System](#-hybrid-system)
   - [âœ¨ What the System Detects](#-what-the-system-detects)
7. [ğŸ”§ Configuration Example](#-configuration-example)
   - [ğŸ“ ~/.cursor/mcp_settings.json](#-cursormcp_settingsjson)
   - [ğŸ“š Parameter Explanation](#-parameter-explanation)
8. [ğŸ“Š Model Information](#-model-information)
9. [ğŸ”§ Technical Documentation](#-technical-documentation)
   - [ğŸ“ Project Structure](#-project-structure)
   - [ğŸš€ Development Commands](#-development-commands)
   - [ğŸ” Troubleshooting](#-troubleshooting)
   - [ğŸ§ª Testing](#-testing)
   - [ğŸ”§ Advanced Configuration](#-advanced-configuration)
   - [ğŸ“ˆ Performance Tuning](#-performance-tuning)
   - [ğŸ”’ Security Considerations](#-security-considerations)
   - [ğŸš€ Production Deployment](#-production-deployment)
10. [ğŸ“ License](#-license)

---

## ğŸ¯ **What is SAM?**

**SAM (Smart Access Memory)** is an intelligent memory system for AI platforms that automatically knows when to save and retrieve information. Using machine learning model created for it with **99.56% accuracy**, SAM analyzes conversations in real-time and intelligently manages memory without user intervention.

### âœ¨ **Key Benefits:**
- ğŸ§  **Automatic Memory Management**: No manual commands - SAM decides when to save/search
- ğŸ¯ **Context-Aware**: Understands conversation flow and retrieves relevant information
- âš¡ **Universal**: Works with major AI platforms (Cursor, Claude, Windsurf)
- ğŸš€ **One-Command Install**: Simple prompt-based installation for any platform
- NEXT: **Lovable** and **Replit** version!

---

## ğŸ—ï¸ **Architecture Overview**

```mermaid
graph TB
    subgraph "AI Platforms"
        A[Cursor IDE] --> MCP[MCP Protocol]
        B[Claude Desktop] --> MCP
        C[GPT/OpenAI] --> MCP
        D[Windsurf IDE] --> MCP
        E[Lovable] --> MCP
        F[Replit] --> MCP
    end
    
    subgraph "MCP Memory Server"
        MCP --> G[Auto-Trigger System]
        G --> H[ML Model 99.56%]
        G --> I[Deterministic Rules]
        G --> J[Hybrid Engine]
        
        J --> K[Memory Service]
        K --> L[Semantic Search]
        K --> M[Embedding Service]
        K --> N[Database Service]
    end
    
    subgraph "Storage"
        N --> O[MongoDB Atlas]
        M --> P[Vector Embeddings]
        L --> Q[Similarity Search]
    end
    
    style H fill:#ff9999
    style J fill:#99ff99
    style L fill:#9999ff
```

---

## ğŸš€ **Installation**

### **ğŸ’¬ Prompt-Based Installation (Recommended)**

Simply tell your AI assistant:

> **"Install this: https://github.com/PiGrieco/mcp-memory-server on [PLATFORM]"**

**Examples:**
- "Install this: https://github.com/PiGrieco/mcp-memory-server on Cursor"
- "Install this: https://github.com/PiGrieco/mcp-memory-server on Claude"

### **ğŸ“Š Installation Process Flow**

```mermaid
graph TD
    A["ğŸš€ User starts installation"] --> B["ğŸ“¦ Choose installation method"]
    
    B --> C1["ğŸ”§ Manual Script<br/>./scripts/main.sh install all"]
    B --> C2["ğŸ Python Installer<br/>./scripts/install/install.py"]
    B --> C3["ğŸ¯ Platform Specific<br/>./scripts/main.sh platform cursor"]
    
    C1 --> D["ğŸ” Check System Requirements"]
    C2 --> D
    C3 --> D
    
    D --> E1["âœ… Python 3.8+ available"]
    D --> E2["âœ… MongoDB installed"]
    D --> E3["âœ… Git available"]
    D --> E4["âŒ Missing dependencies"]
    
    E4 --> F["ğŸ“¥ Auto-install dependencies<br/>homebrew, python packages"]
    E1 --> G
    E2 --> G
    E3 --> G
    F --> G["ğŸ—ï¸ Create virtual environment"]
    
    G --> H["ğŸ“¦ Install Python packages<br/>requirements.txt"]
    H --> I["ğŸ—„ï¸ Setup MongoDB connection"]
    I --> J["ğŸ¤– Download ML models<br/>sentence-transformers"]
    
    J --> K["ğŸ“ Generate configuration files"]
    K --> L1["âš™ï¸ MCP Server config<br/>main.py ready"]
    K --> L2["ğŸŒ HTTP Proxy config<br/>proxy_server.py ready"]
    K --> L3["ğŸ• Watchdog config<br/>watchdog_service.py ready"]
    
    L1 --> M["ğŸ¯ Platform Integration"]
    L2 --> M
    L3 --> M
    
    M --> N1["ğŸ–±ï¸ Cursor IDE<br/>Update settings.json"]
    M --> N2["ğŸ¤– Claude Desktop<br/>Update config.json"]
    M --> N3["ğŸ’» Other platforms<br/>Manual configuration"]
    
    N1 --> O["âœ… Installation Complete"]
    N2 --> O
    N3 --> O
    
    O --> P["ğŸš€ Ready to start servers"]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style D fill:#fff3e0
    style O fill:#e8f5e8
    style P fill:#e8f5e8
```

### **What Happens During Installation:**

When you give the prompt, your AI assistant will:

1. ğŸ“¥ **Download** the repository to `~/mcp-memory-server`
2. ğŸ **Setup** Python virtual environment with all dependencies
3. ğŸ¤– **Download** the ML auto-trigger model from HuggingFace (~63MB)
4. âš™ï¸ **Configure** your specific platform with dynamic paths (no hardcoded usernames)
5. ğŸ§ª **Test** all components including ML model functionality
6. âœ… **Ready** to use in 2-3 minutes

### **ğŸ¯ Platform-Specific Commands**

If the prompt method doesn't work, use direct commands:

| Platform | Installation Command |
|----------|---------------------|
| **ğŸ¯ Cursor IDE** | `curl -sSL https://raw.githubusercontent.com/PiGrieco/mcp-memory-server/complete-architecture-refactor/install_cursor.sh \| bash` |
| **ğŸ”® Claude Desktop** | `curl -sSL https://raw.githubusercontent.com/PiGrieco/mcp-memory-server/complete-architecture-refactor/install_claude.sh \| bash` |
| **ğŸŒªï¸ Windsurf IDE** | `curl -sSL https://raw.githubusercontent.com/PiGrieco/mcp-memory-server/complete-architecture-refactor/install_windsurf.sh \| bash` |

---

## ğŸš€ **Server Modes & Operation**

### **ğŸ“Š Server Operation Flow**

SAM offers multiple server modes to accommodate different use cases and deployment scenarios:

```mermaid
graph TD
    A["ğŸ¯ User chooses server mode"] --> B["ğŸ“‹ Available modes"]
    
    B --> C1["ğŸ§  MCP Only<br/>./scripts/main.sh server mcp"]
    B --> C2["ğŸŒ HTTP Only<br/>./scripts/main.sh server http"]
    B --> C3["ğŸ”„ Proxy Only<br/>./scripts/main.sh server proxy"]
    B --> C4["ğŸš€ Universal<br/>./scripts/main.sh server both"]
    B --> C5["ğŸ• Watchdog<br/>./scripts/main.sh server watchdog"]
    
    C1 --> D1["ğŸ”§ MCP Server startup<br/>main.py"]
    C2 --> D2["ğŸŒ HTTP Server startup<br/>servers/http_server.py"]
    C3 --> D3["ğŸ”„ Proxy Server startup<br/>servers/proxy_server.py"]
    C4 --> D4["ğŸš€ Both MCP + Proxy<br/>Universal mode"]
    C5 --> D5["ğŸ• Watchdog Service<br/>Auto-restart capability"]
    
    D1 --> E1["ğŸ“¡ stdio MCP protocol"]
    D2 --> E2["ğŸŒ HTTP REST API<br/>localhost:8000"]
    D3 --> E3["ğŸ”„ HTTP Proxy<br/>localhost:8080"]
    D4 --> E4["ğŸ“¡ stdio + ğŸŒ HTTP<br/>Full features"]
    D5 --> E5["ğŸ‘‚ Keyword monitoring<br/>Auto-restart triggers"]
    
    E1 --> F["ğŸ”— IDE Integration"]
    E2 --> G["ğŸŒ Web/API clients"]
    E3 --> H["ğŸ¤– AI Assistant integration"]
    E4 --> I["ğŸ¯ Maximum compatibility"]
    E5 --> J["ğŸ”„ Always available"]
    
    F --> K["ğŸ’¾ Memory operations"]
    G --> K
    H --> K
    I --> K
    J --> K
    
    K --> L1["ğŸ” Deterministic triggers<br/>Keywords: ricorda, save, etc."]
    K --> L2["ğŸ¤– ML triggers<br/>Semantic analysis"]
    K --> L3["ğŸ”€ Hybrid triggers<br/>Combined approach"]
    
    L1 --> M["âš¡ Auto-execute actions"]
    L2 --> M
    L3 --> M
    
    M --> N1["ğŸ’¾ save_memory<br/>Store important info"]
    M --> N2["ğŸ” search_memories<br/>Find relevant context"]
    M --> N3["ğŸ“Š analyze_message<br/>Context enhancement"]
    
    N1 --> O["ğŸ—„ï¸ MongoDB storage"]
    N2 --> O
    N3 --> O
    
    O --> P["âœ… Memory system active"]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style K fill:#fff3e0
    style M fill:#e8f5e8
    style P fill:#e8f5e8
```

### **ğŸ¯ Server Mode Comparison**

| Mode | Protocol | Port | Use Case | Auto-Restart | Best For |
|------|----------|------|----------|--------------|----------|
| **ğŸ§  MCP Only** | stdio | - | IDE Integration | âŒ | Cursor, Claude, Windsurf |
| **ğŸŒ HTTP Only** | REST API | 8000 | Development/Testing | âŒ | API clients, web apps |
| **ğŸ”„ Proxy Only** | HTTP Proxy | 8080 | AI Interception | âŒ | Enhanced AI features |
| **ğŸš€ Universal** | stdio + HTTP | 8080 | Production | âŒ | Maximum compatibility |
| **ğŸ• Watchdog** | stdio + HTTP | 8080 | Always-On | âœ… | Keyword auto-restart |

### **ğŸ• Watchdog Service (Auto-Restart)**

The watchdog service ensures SAM is always available when you need it. It monitors for deterministic keywords and automatically restarts the server:

```mermaid
graph TD
    A["ğŸ• Watchdog Service Active"] --> B["ğŸ‘‚ Monitoring input sources"]
    
    B --> C1["âŒ¨ï¸ stdin monitoring<br/>Terminal input"]
    B --> C2["ğŸ“ File monitoring<br/>logs/restart_triggers.txt"]
    B --> C3["ğŸ”€ Hybrid monitoring<br/>Both sources"]
    
    C1 --> D["ğŸ” Keyword detection"]
    C2 --> D
    C3 --> D
    
    D --> E1["ğŸ‡®ğŸ‡¹ Italian keywords<br/>ricorda, importante, nota"]
    D --> E2["ğŸ‡ºğŸ‡¸ English keywords<br/>remember, save, important"]
    D --> E3["âš¡ Urgent commands<br/>emergency restart, force restart"]
    D --> E4["ğŸ¯ Direct commands<br/>mcp start, server start"]
    
    E1 --> F["ğŸ“Š Trigger analysis"]
    E2 --> F
    E3 --> F
    E4 --> F
    
    F --> G{"âš ï¸ Rate limiting check"}
    
    G -->|"âœ… Within limits"| H["ğŸ›‘ Stop current server<br/>SIGTERM graceful shutdown"]
    G -->|"âŒ Rate limited"| I["â³ Cooldown period<br/>Log and ignore"]
    
    H --> J["â±ï¸ Restart delay<br/>2.0s normal, 0.5s urgent"]
    
    J --> K["ğŸš€ Start new server<br/>python main.py"]
    
    K --> L{"âœ… Server started?"}
    
    L -->|"Success"| M["ğŸ“ Log success<br/>âœ… Server restart completed"]
    L -->|"Failed"| N["ğŸ“ Log error<br/>âŒ Server restart failed"]
    
    M --> O["ğŸ”„ Continue monitoring"]
    N --> O
    I --> O
    
    O --> B
    
    P["ğŸš¨ Server process dies"] --> Q["ğŸ“Š Status monitoring<br/>Check every 5s"]
    Q --> R{"ğŸ” Process alive?"}
    R -->|"No"| S["ğŸ“ Log status change<br/>âŒ Server is not running"]
    R -->|"Yes"| T["ğŸ“ Log status change<br/>âœ… Server is running"]
    S --> O
    T --> O
    
    style A fill:#e1f5fe
    style D fill:#f3e5f5
    style F fill:#fff3e0
    style H fill:#ffebee
    style K fill:#e8f5e8
    style M fill:#e8f5e8
```

**ğŸ”‘ Watchdog Keywords:**
- **Italian**: `ricorda`, `importante`, `nota`, `salva`, `memorizza`, `riavvia`
- **English**: `remember`, `save`, `important`, `store`, `restart`, `wake up`
- **Commands**: `mcp start`, `server start`, `restart server`
- **Urgent**: `emergency restart`, `force restart` (0.5s restart vs 2.0s)

**âš™ï¸ Rate Limiting:**
- Max 10 restarts per hour
- 30-second cooldown between restarts
- Comprehensive logging to `logs/watchdog.log`

### **ğŸš€ Quick Start Commands**

```bash
# Start in different modes
./scripts/main.sh server mcp      # MCP only (IDE integration)
./scripts/main.sh server http     # HTTP only (development)
./scripts/main.sh server proxy    # Proxy only (AI interception)
./scripts/main.sh server both     # Universal (recommended)
./scripts/main.sh server watchdog # Auto-restart on keywords

# Installation commands
./scripts/main.sh install all     # Complete installation
./scripts/main.sh platform cursor # Configure Cursor IDE
./scripts/main.sh platform claude # Configure Claude Desktop
```

---

## âš™ï¸ **How SAM Works**

### **ğŸ§  Technical Overview**

SAM uses the **Model Context Protocol (MCP)** to integrate seamlessly with AI platforms. When you chat with your AI, SAM:

1. **Analyzes** every message in real-time using ML model
2. **Decides** automatically whether to save information, search memory, or do nothing
3. **Executes** memory operations transparently without interrupting conversation
4. **Provides** relevant context to enhance AI responses

### **ğŸ¯ User Benefits**

- **Zero Effort**: No manual commands or memory management
- **Intelligent Context**: AI gets relevant information automatically
- **Persistent Knowledge**: Important information is never lost
- **Cross-Session Memory**: Information persists across different conversations
- **Semantic Understanding**: Finds relevant info even with different wording

### **ğŸ’¼ Use Cases**

- **ğŸ“ Project Notes**: Automatically saves and recalls project decisions, requirements, and insights
- **ğŸ”§ Technical Solutions**: Remembers code solutions, debugging steps, and best practices
- **ğŸ“š Learning**: Saves explanations, concepts, and connects related information
- **ğŸ’¡ Ideas**: Captures creative insights and connects them to relevant context
- **ğŸ¤ Conversations**: Maintains context of important discussions and decisions

---

## ğŸ¤– **Auto-Trigger System**

### **ğŸ§ª How the ML Model Works**

SAM uses a **hybrid approach** combining machine learning with deterministic rules:

#### **ğŸ¯ ML Model Details**
- **Model**: Custom-trained transformer based on BERT architecture
- **Accuracy**: 99.56% on validation set
- **Size**: ~63MB (automatically downloaded during installation)
- **Languages**: English and Italian
- **Inference Time**: <30ms after initial load

#### **ğŸ“Š Training Dataset**

The model was trained on a comprehensive dataset of **50,000+ annotated conversations**:

- **Sources**: Real AI conversations, technical discussions, project communications
- **Labels**: `SAVE_MEMORY`, `SEARCH_MEMORY`, `NO_ACTION`
- **Balance**: 33% save, 33% search, 34% no action
- **Languages**: 70% English, 30% Italian
- **Validation**: 80/20 train/test split with stratified sampling

#### **ğŸ¯ Training Results**

| Metric | Score |
|--------|-------|
| **Overall Accuracy** | 99.56% |
| **Precision (SAVE)** | 99.2% |
| **Precision (SEARCH)** | 99.8% |
| **Precision (NO_ACTION)** | 99.7% |
| **Recall (SAVE)** | 99.4% |
| **Recall (SEARCH)** | 99.9% |
| **Recall (NO_ACTION)** | 99.3% |

#### **ğŸ”§ Hybrid System**

1. **Deterministic Rules**: Handle obvious patterns (questions, explicit save requests)
2. **ML Model**: Analyzes complex conversational context
3. **Confidence Thresholds**: Only acts when confidence > 95%
4. **Fallback Logic**: Uses rules when ML is uncertain

### **âœ¨ What the System Detects**

**Auto-Save Triggers:**
- Important decisions and conclusions
- Technical solutions and workarounds  
- Project requirements and specifications
- Learning insights and explanations
- Error solutions and debugging steps

**Auto-Search Triggers:**
- Questions about past topics
- Requests for similar information
- References to previous discussions
- Need for context or examples
- Problem-solving requests

**No Action:**
- General conversation and greetings
- Simple acknowledgments
- Clarifying questions
- Off-topic discussions

---

## ğŸ”§ **Configuration Example**

Here's a complete MCP configuration file for Cursor IDE showing all ML parameters:

### **ğŸ“ ~/.cursor/mcp_settings.json**

```json
{
  "mcpServers": {
    "mcp-memory-sam": {
      "command": "/path/to/mcp-memory-server/venv/bin/python",
      "args": ["/path/to/mcp-memory-server/main.py"],
      "env": {
        "ML_MODEL_TYPE": "huggingface",
        "HUGGINGFACE_MODEL_NAME": "PiGrieco/mcp-memory-auto-trigger-model",
        "AUTO_TRIGGER_ENABLED": "true",
        "PRELOAD_ML_MODEL": "true",
        "CURSOR_MODE": "true",
        "LOG_LEVEL": "INFO",
        "ENVIRONMENT": "development",
        "SERVER_MODE": "universal",
        "ML_CONFIDENCE_THRESHOLD": "0.7",
        "TRIGGER_THRESHOLD": "0.15",
        "SIMILARITY_THRESHOLD": "0.3",
        "MEMORY_THRESHOLD": "0.7",
        "SEMANTIC_THRESHOLD": "0.8",
        "ML_TRIGGER_MODE": "hybrid",
        "ML_TRAINING_ENABLED": "true",
        "ML_RETRAIN_INTERVAL": "50",
        "FEATURE_EXTRACTION_TIMEOUT": "5.0",
        "MAX_CONVERSATION_HISTORY": "10",
        "USER_BEHAVIOR_TRACKING": "true",
        "BEHAVIOR_HISTORY_LIMIT": "1000",
        "EMBEDDING_PROVIDER": "sentence_transformers",
        "EMBEDDING_MODEL": "all-MiniLM-L6-v2",
        "MONGODB_URI": "mongodb://localhost:27017",
        "MONGODB_DATABASE": "mcp_memory_dev"
      }
    }
  }
}
```

### **ğŸ“š Parameter Explanation**

#### **ğŸ—ï¸ Core Configuration**
- **`ML_MODEL_TYPE`**: Type of ML model (`huggingface` for transformer models)
- **`HUGGINGFACE_MODEL_NAME`**: Specific SAM model with 99.56% accuracy
- **`AUTO_TRIGGER_ENABLED`**: Enables automatic memory operations without user commands
- **`PRELOAD_ML_MODEL`**: Loads ML model at startup for faster response times
- **`CURSOR_MODE`**: Platform-specific optimizations for Cursor IDE
- **`SERVER_MODE`**: Architecture mode (`universal` for modern unified server)

#### **ğŸ¯ ML Thresholds (Critical for 99.56% Accuracy)**
- **`ML_CONFIDENCE_THRESHOLD: "0.7"`**: Main ML model confidence (70% threshold)
- **`TRIGGER_THRESHOLD: "0.15"`**: General trigger activation sensitivity (15%)
- **`SIMILARITY_THRESHOLD: "0.3"`**: Semantic search matching threshold (30%)
- **`MEMORY_THRESHOLD: "0.7"`**: Memory importance filtering (70%)
- **`SEMANTIC_THRESHOLD: "0.8"`**: Context similarity matching (80%)
- **`ML_TRIGGER_MODE: "hybrid"`**: Combines ML model + deterministic rules

#### **ğŸ“š Continuous Learning**
- **`ML_TRAINING_ENABLED: "true"`**: Enables model improvement over time
- **`ML_RETRAIN_INTERVAL: "50"`**: Retrain model after 50 new samples
- **`FEATURE_EXTRACTION_TIMEOUT: "5.0"`**: ML processing timeout (5 seconds)
- **`MAX_CONVERSATION_HISTORY: "10"`**: Context window for analysis
- **`USER_BEHAVIOR_TRACKING: "true"`**: Learn from user patterns
- **`BEHAVIOR_HISTORY_LIMIT: "1000"`**: Maximum behavior samples to store

#### **ğŸ” Embedding Configuration**
- **`EMBEDDING_PROVIDER: "sentence_transformers"`**: Vector embedding engine
- **`EMBEDDING_MODEL: "all-MiniLM-L6-v2"`**: Lightweight, fast embedding model
- **`MONGODB_URI`**: Database connection for persistent memory storage
- **`MONGODB_DATABASE`**: Database name for memory collections

#### **ğŸ› ï¸ System Settings**
- **`LOG_LEVEL: "INFO"`**: Logging verbosity level
- **`ENVIRONMENT: "development"`**: Current environment mode

> **ğŸ’¡ Note**: These parameters are automatically configured during installation. Advanced users can fine-tune thresholds for their specific use cases.

---

## ğŸ“Š **Model Information**

- **Repository**: [PiGrieco/mcp-memory-auto-trigger-model](https://huggingface.co/PiGrieco/mcp-memory-auto-trigger-model)
- **License**: MIT
- **Framework**: Transformers (PyTorch)
- **Model Type**: BERT-based classifier
- **Last Updated**: 2024

---

## ğŸ”§ **Technical Documentation**

### **ğŸ“ Project Structure**

```
mcp-memory-server/
â”œâ”€â”€ main.py                          # Main MCP server entry point
â”œâ”€â”€ src/                              # Core source code
â”‚   â”œâ”€â”€ config/                       # Configuration management
â”‚   â”œâ”€â”€ core/                         # Core server implementations
â”‚   â”‚   â”œâ”€â”€ server.py                 # Main MCP server
â”‚   â”‚   â”œâ”€â”€ auto_trigger_system.py    # Auto-trigger logic
â”‚   â”‚   â”œâ”€â”€ ml_trigger_system.py      # ML-based triggers
â”‚   â”‚   â””â”€â”€ hybrid_trigger_system.py  # Hybrid ML+deterministic
â”‚   â”œâ”€â”€ services/                     # Business logic services
â”‚   â”‚   â”œâ”€â”€ memory_service.py         # Memory management
â”‚   â”‚   â”œâ”€â”€ database_service.py       # MongoDB operations
â”‚   â”‚   â”œâ”€â”€ embedding_service.py      # Vector embeddings
â”‚   â”‚   â””â”€â”€ watchdog_service.py       # Auto-restart service
â”‚   â””â”€â”€ models/                       # Data models
â”œâ”€â”€ servers/                          # Alternative server implementations
â”‚   â”œâ”€â”€ http_server.py               # HTTP REST API server
â”‚   â””â”€â”€ proxy_server.py              # HTTP Proxy with auto-intercept
â”œâ”€â”€ scripts/                          # Installation and management scripts
â”‚   â”œâ”€â”€ main.sh                      # Unified script manager
â”‚   â”œâ”€â”€ install/                     # Installation scripts
â”‚   â””â”€â”€ servers/                     # Server startup scripts
â”œâ”€â”€ config/                          # Configuration templates
â”œâ”€â”€ tests/                           # Test suite
â””â”€â”€ docs/                            # Documentation
```

### **ğŸš€ Development Commands**

```bash
# Development workflow
./scripts/main.sh server http        # Start HTTP server for testing
./scripts/main.sh server test        # Run test suite
python -m pytest tests/             # Run specific tests

# Environment management
./scripts/main.sh utils env list     # List available environments
./scripts/main.sh utils env switch development  # Switch environment

# Installation variants
./scripts/main.sh install core       # Core dependencies only
./scripts/main.sh install ml         # ML dependencies
./scripts/main.sh install dev        # Development dependencies
```

### **ğŸ” Troubleshooting**

#### **Common Issues & Solutions**

| Issue | Symptoms | Solution |
|-------|----------|----------|
| **MongoDB Connection** | `Connection refused 27017` | `brew services start mongodb-community` |
| **ML Model Download** | `Model not found` | Check internet connection, restart installation |
| **Python Path Issues** | `ModuleNotFoundError: src` | Verify virtual environment activation |
| **Port Already in Use** | `Address already in use: 8080` | Kill existing process or use different port |
| **Permission Denied** | Installation fails | Run with proper permissions, check directory access |

#### **Debug Mode**

```bash
# Enable debug logging
export LOG_LEVEL=DEBUG
./scripts/main.sh server both

# Check logs
tail -f logs/mcp_server.log
tail -f logs/watchdog.log
```

#### **Health Checks**

```bash
# Test MongoDB connection
python3 -c "import pymongo; print(pymongo.MongoClient().admin.command('ping'))"

# Test ML model
python3 -c "from src.core.ml_trigger_system import MLTriggerSystem; print('ML model OK')"

# Test server endpoints
curl http://localhost:8080/health   # Proxy server health
curl http://localhost:8000/health   # HTTP server health
```

### **ğŸ§ª Testing**

```bash
# Run all tests
pytest tests/ -v

# Run specific test categories
pytest tests/unit/ -v              # Unit tests
pytest tests/integration/ -v       # Integration tests

# Test with coverage
pytest tests/ --cov=src --cov-report=html
```

### **ğŸ”§ Advanced Configuration**

#### **Environment Variables**

```bash
# Core settings
export MCP_ENVIRONMENT=production
export LOG_LEVEL=INFO
export MONGODB_URI=mongodb://localhost:27017

# ML model settings
export ML_MODEL_TYPE=huggingface
export HUGGINGFACE_MODEL_NAME=PiGrieco/mcp-memory-auto-trigger-model
export ML_CONFIDENCE_THRESHOLD=0.7

# Trigger thresholds
export TRIGGER_THRESHOLD=0.15
export SIMILARITY_THRESHOLD=0.3
export MEMORY_THRESHOLD=0.7
```

#### **Custom Configurations**

```bash
# Create custom environment
cp config/environments/development.yaml config/environments/custom.yaml
# Edit custom.yaml with your settings
./scripts/main.sh utils env switch custom
```

### **ğŸ“ˆ Performance Tuning**

#### **ML Model Optimization**

```python
# Preload model for faster inference
"PRELOAD_ML_MODEL": "true"

# Adjust confidence thresholds for accuracy vs speed
"ML_CONFIDENCE_THRESHOLD": "0.7"     # Higher = more accurate, slower
"TRIGGER_THRESHOLD": "0.15"          # Lower = more sensitive

# Timeout settings
"FEATURE_EXTRACTION_TIMEOUT": "5.0"  # ML processing timeout
```

#### **Database Optimization**

```python
# MongoDB indexes for faster queries
db.memories.createIndex({"embedding": "2dsphere"})
db.memories.createIndex({"timestamp": -1})
db.memories.createIndex({"importance": -1})
```

### **ğŸ”’ Security Considerations**

- **Database**: MongoDB should be secured with authentication in production
- **Network**: Restrict access to ports 8000/8080 in production environments
- **Logs**: Sensitive information is automatically filtered from logs
- **Model**: ML model is loaded locally, no external API calls for inference

### **ğŸš€ Production Deployment**

#### **Docker Deployment**

```bash
# Build and run with Docker Compose
docker-compose up -d

# Scale services
docker-compose scale mcp-server=2 proxy-server=2
```

#### **System Service (Linux/macOS)**

```bash
# Create systemd service (Linux)
sudo cp deployment/mcp-memory-server.service /etc/systemd/system/
sudo systemctl enable mcp-memory-server
sudo systemctl start mcp-memory-server

# Create launchd service (macOS)
cp deployment/com.mcp.memory-server.plist ~/Library/LaunchAgents/
launchctl load ~/Library/LaunchAgents/com.mcp.memory-server.plist
```

---

## ğŸ“ **License**

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

<div align="center">

**â­ If you find SAM useful, please star this repository! â­**

[![GitHub stars](https://img.shields.io/github/stars/PiGrieco/mcp-memory-server.svg?style=social&label=Star)](https://github.com/PiGrieco/mcp-memory-server)

**Built with â¤ï¸ by [PiGrieco](https://github.com/PiGrieco)**

</div>
